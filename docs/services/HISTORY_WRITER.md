# üìù History Writer ‚Äî –î–µ—Ç–∞–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

> **–ë–ª–æ–∫:** 1 (Data Collection)  
> **–ü–æ—Ä—Ç:** HTTP 8091 (admin/metrics)  
> **–°–ª–æ–∂–Ω–æ—Å—Ç—å:** –°—Ä–µ–¥–Ω—è—è  
> **–°—Ç–∞—Ç—É—Å:** üü° –í —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ

---

## üìã –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. [–û–±–∑–æ—Ä](#–æ–±–∑–æ—Ä)
2. [–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞](#–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞)
3. [Kafka Consumer](#kafka-consumer)
4. [Batch Processing](#batch-processing)
5. [TimescaleDB –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è](#timescaledb-–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è)
6. [–û—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å](#–æ—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å)
7. [–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ](#–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ)
8. [–ú–µ—Ç—Ä–∏–∫–∏ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥](#–º–µ—Ç—Ä–∏–∫–∏-–∏-–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥)
9. [–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è](#–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è)

---

## –û–±–∑–æ—Ä

**History Writer** ‚Äî —Å–µ—Ä–≤–∏—Å –∑–∞–ø–∏—Å–∏ GPS –∏—Å—Ç–æ—Ä–∏–∏ –≤ TimescaleDB. –ß–∏—Ç–∞–µ—Ç —Å–æ–±—ã—Ç–∏—è –∏–∑ Kafka, –±—É—Ñ–µ—Ä–∏–∑—É–µ—Ç –∏—Ö –∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç batch insert –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.

### –ö–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –ó–Ω–∞—á–µ–Ω–∏–µ |
|----------|----------|
| **–í—Ö–æ–¥** | Kafka —Ç–æ–ø–∏–∫ `gps-events` |
| **–í—ã—Ö–æ–¥** | TimescaleDB —Ç–∞–±–ª–∏—Ü–∞ `gps_points` |
| **Batch size** | 500 —Ç–æ—á–µ–∫ (–Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è) |
| **Flush interval** | 5 —Å–µ–∫—É–Ω–¥ (max) |
| **–ü—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å** | 10,000+ —Ç–æ—á–µ–∫/—Å–µ–∫ |
| **Latency** | < 10 —Å–µ–∫—É–Ω–¥ (Kafka ‚Üí DB) |

### –ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã

```
Kafka (gps-events) ‚Üí Buffer (in-memory) ‚Üí Batch Insert ‚Üí TimescaleDB
                          ‚Üì
                     Flush trigger:
                     - Buffer >= 500 points
                     - Time >= 5 seconds
                     - Shutdown signal
```

---

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```mermaid
flowchart TB
    subgraph Kafka["Apache Kafka"]
        Topic[gps-events<br/>12 partitions]
    end

    subgraph HW["History Writer Cluster"]
        subgraph HW1["Instance 1"]
            C1[Consumer]
            B1[Buffer]
            W1[Writer]
        end
        subgraph HW2["Instance 2"]
            C2[Consumer]
            B2[Buffer]
            W2[Writer]
        end
        subgraph HW3["Instance 3"]
            C3[Consumer]
            B3[Buffer]
            W3[Writer]
        end
    end

    subgraph DB["TimescaleDB"]
        GP[gps_points<br/>hypertable]
    end

    Topic --> |partitions 0-3| C1
    Topic --> |partitions 4-7| C2
    Topic --> |partitions 8-11| C3

    C1 --> B1 --> W1
    C2 --> B2 --> W2
    C3 --> B3 --> W3

    W1 & W2 & W3 --> GP
```

### –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –û–ø–∏—Å–∞–Ω–∏–µ | –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—è |
|-----------|----------|------------|
| **Kafka Consumer** | –ß—Ç–µ–Ω–∏–µ GPS —Å–æ–±—ã—Ç–∏–π | zio-kafka |
| **Buffer** | In-memory –±—É—Ñ–µ—Ä —Ç–æ—á–µ–∫ | ZIO Queue |
| **Batch Writer** | Batch insert –≤ –ë–î | Doobie + COPY |
| **Flush Controller** | –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ flush | ZIO Schedule |
| **Admin API** | Health, metrics | ZIO HTTP |

---

## Kafka Consumer

### Consumer Group

```mermaid
sequenceDiagram
    participant K as Kafka
    participant CG as Consumer Group
    participant HW1 as History Writer 1
    participant HW2 as History Writer 2
    participant HW3 as History Writer 3

    Note over K,HW3: Consumer Group: history-writer-group

    K->>CG: 12 partitions available
    CG->>HW1: Assign partitions 0,1,2,3
    CG->>HW2: Assign partitions 4,5,6,7
    CG->>HW3: Assign partitions 8,9,10,11

    loop Consume
        K->>HW1: Poll (partitions 0-3)
        K->>HW2: Poll (partitions 4-7)
        K->>HW3: Poll (partitions 8-11)
        HW1->>HW1: Buffer points
        HW2->>HW2: Buffer points
        HW3->>HW3: Buffer points
    end

    Note over HW1,HW3: Rebalance –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏/—É–¥–∞–ª–µ–Ω–∏–∏ instance
```

### Consumer –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

```scala
val consumerSettings = ConsumerSettings(List("kafka:9092"))
  .withGroupId("history-writer-group")
  .withClientId(s"history-writer-${instanceId}")
  .withOffsetRetrieval(OffsetRetrieval.Auto(AutoOffsetStrategy.Earliest))
  .withProperty("enable.auto.commit", "false")  // Manual commit!
  .withProperty("max.poll.records", "1000")
  .withProperty("fetch.min.bytes", "1024")
  .withProperty("fetch.max.wait.ms", "500")
  .withProperty("session.timeout.ms", "30000")
  .withProperty("heartbeat.interval.ms", "10000")
```

### –ß—Ç–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π

```scala
def consumeGpsEvents: ZStream[Consumer, Throwable, GpsEvent] =
  Consumer
    .subscribeAnd(Subscription.topics("gps-events"))
    .plainStream(Serde.string, GpsEventSerde)
    .tap(record => 
      ZIO.logDebug(s"Received: vehicleId=${record.value.vehicleId}")
    )
    .map(_.value)
```

---

## Batch Processing

### Buffer Strategy

```mermaid
flowchart TB
    subgraph Input["–í—Ö–æ–¥—è—â–∏–µ —Ç–æ—á–∫–∏"]
        P1[Point 1]
        P2[Point 2]
        PN[Point N]
    end

    subgraph Buffer["In-Memory Buffer"]
        Q[ZIO Queue<br/>capacity: 10000]
        Count[Count: 487]
        Timer[Timer: 3.2s]
    end

    subgraph Trigger["Flush Triggers"]
        T1{Count >= 500?}
        T2{Time >= 5s?}
        T3{Shutdown?}
    end

    subgraph Flush["Flush Process"]
        Take[Take batch from queue]
        Insert[Batch INSERT]
        Commit[Commit offsets]
    end

    P1 & P2 & PN --> Q
    Q --> T1 & T2 & T3
    T1 -->|Yes| Take
    T2 -->|Yes| Take
    T3 -->|Yes| Take
    Take --> Insert --> Commit
```

### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –±—É—Ñ–µ—Ä–∞

```scala
case class BufferState(
  points: Chunk[GpsEvent],
  offsets: Map[TopicPartition, Long],
  lastFlush: Instant
)

class BatchBuffer(
  queue: Queue[GpsEvent],
  state: Ref[BufferState],
  config: BufferConfig
) {
  
  // –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–æ—á–∫–∏ –≤ –±—É—Ñ–µ—Ä
  def add(event: GpsEvent, offset: CommittableOffset): UIO[Unit] =
    for {
      _ <- queue.offer(event)
      _ <- state.update(s => s.copy(
        offsets = s.offsets + (offset.topicPartition -> offset.offset)
      ))
    } yield ()
  
  // –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ª–æ–≤–∏–π flush
  def shouldFlush: UIO[Boolean] =
    for {
      s <- state.get
      size <- queue.size
      now <- Clock.instant
      timeSinceFlush = Duration.between(s.lastFlush, now)
    } yield size >= config.batchSize || 
            timeSinceFlush >= config.flushInterval
  
  // –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ batch –¥–ª—è –∑–∞–ø–∏—Å–∏
  def takeBatch: UIO[Chunk[GpsEvent]] =
    queue.takeUpTo(config.batchSize)
  
  // –ü–æ–ª—É—á–µ–Ω–∏–µ offsets –¥–ª—è commit
  def getOffsets: UIO[Map[TopicPartition, Long]] =
    state.get.map(_.offsets)
  
  // –û—á–∏—Å—Ç–∫–∞ –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ flush
  def clearOffsets: UIO[Unit] =
    state.update(s => s.copy(
      offsets = Map.empty,
      lastFlush = Instant.now()
    ))
}

case class BufferConfig(
  batchSize: Int = 500,
  flushInterval: Duration = 5.seconds,
  maxQueueSize: Int = 10000
)
```

### Flush Controller

```scala
object FlushController {
  
  def run(
    buffer: BatchBuffer,
    writer: BatchWriter,
    consumer: Consumer
  ): ZIO[Any, Throwable, Unit] = {
    
    val flushLoop = for {
      shouldFlush <- buffer.shouldFlush
      _ <- ZIO.when(shouldFlush) {
        for {
          batch <- buffer.takeBatch
          _ <- ZIO.logInfo(s"Flushing ${batch.size} points")
          
          // –ó–∞–ø–∏—Å—å –≤ –ë–î
          _ <- writer.writeBatch(batch)
            .retry(Schedule.exponential(100.millis) && Schedule.recurs(3))
          
          // Commit offsets –≤ Kafka
          offsets <- buffer.getOffsets
          _ <- consumer.commit(offsets)
          
          // –û—á–∏—Å—Ç–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è
          _ <- buffer.clearOffsets
          
          _ <- ZIO.logInfo(s"Flush complete, committed offsets")
        } yield ()
      }
    } yield ()
    
    // –ó–∞–ø—É—Å–∫ loop –∫–∞–∂–¥—ã–µ 100ms
    flushLoop
      .repeat(Schedule.spaced(100.millis))
      .forever
  }
}
```

---

## TimescaleDB –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è

### –°—Ö–µ–º–∞ —Ç–∞–±–ª–∏—Ü—ã

```sql
-- –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ GPS —Ç–æ—á–µ–∫ (hypertable)
CREATE TABLE gps_points (
  -- –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è
  time TIMESTAMPTZ NOT NULL,
  vehicle_id BIGINT NOT NULL,
  
  -- –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
  latitude DOUBLE PRECISION NOT NULL,
  longitude DOUBLE PRECISION NOT NULL,
  altitude INTEGER,
  speed INTEGER NOT NULL DEFAULT 0,
  course INTEGER,
  satellites INTEGER,
  
  -- –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
  protocol VARCHAR(20) NOT NULL,
  is_valid BOOLEAN NOT NULL DEFAULT true,
  validation_error VARCHAR(50),
  is_moving BOOLEAN NOT NULL DEFAULT true,
  
  -- –î–∞—Ç—á–∏–∫–∏ (JSONB –¥–ª—è –≥–∏–±–∫–æ—Å—Ç–∏)
  sensors JSONB,
  
  -- –°–ª—É–∂–µ–±–Ω—ã–µ
  received_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  instance_id VARCHAR(50),
  
  -- Primary key –¥–ª—è TimescaleDB
  PRIMARY KEY (time, vehicle_id)
);

-- –°–æ–∑–¥–∞–Ω–∏–µ hypertable (–ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏)
SELECT create_hypertable('gps_points', 'time',
  chunk_time_interval => INTERVAL '1 day',
  if_not_exists => TRUE
);

-- –ò–Ω–¥–µ–∫—Å—ã
CREATE INDEX idx_gps_points_vehicle_time 
  ON gps_points (vehicle_id, time DESC);

CREATE INDEX idx_gps_points_sensors 
  ON gps_points USING GIN (sensors);

-- –ü–æ–ª–∏—Ç–∏–∫–∞ —Å–∂–∞—Ç–∏—è (–ø–æ—Å–ª–µ 7 –¥–Ω–µ–π)
ALTER TABLE gps_points SET (
  timescaledb.compress,
  timescaledb.compress_segmentby = 'vehicle_id'
);

SELECT add_compression_policy('gps_points', INTERVAL '7 days');

-- –ü–æ–ª–∏—Ç–∏–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è (–ø–æ—Å–ª–µ 1 –≥–æ–¥–∞)
SELECT add_retention_policy('gps_points', INTERVAL '1 year');
```

### Batch Insert (COPY)

```scala
object BatchWriter {
  
  // –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ COPY –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
  def writeBatch(points: Chunk[GpsEvent]): ZIO[Transactor, Throwable, Int] =
    ZIO.serviceWithZIO[Transactor] { xa =>
      
      val copyIn = 
        """COPY gps_points (
          time, vehicle_id, latitude, longitude, altitude,
          speed, course, satellites, protocol, is_valid,
          validation_error, is_moving, sensors, received_at, instance_id
        ) FROM STDIN WITH (FORMAT csv)"""
      
      // –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ CSV —Å—Ç—Ä–æ–∫–∏
      val csvData = points.map(pointToCsv).mkString("\n")
      
      // –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ COPY
      HC.pgGetCopyAPI(PFCM.copyIn(copyIn, new StringReader(csvData)))
        .transact(xa)
    }
  
  private def pointToCsv(p: GpsEvent): String = {
    val sensors = p.sensors.map(_.noSpaces).getOrElse("")
    s"${p.deviceTime},${p.vehicleId},${p.latitude},${p.longitude}," +
    s"${p.altitude.getOrElse("")},${p.speed},${p.course.getOrElse("")}," +
    s"${p.satellites.getOrElse("")},${p.protocol},${p.isValid}," +
    s"${p.validationError.getOrElse("")},${p.isMoving}," +
    s"\"${sensors}\",${p.serverTime},${p.instanceId}"
  }
  
  // –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: Batch INSERT (–º–µ–¥–ª–µ–Ω–Ω–µ–µ, –Ω–æ –ø—Ä–æ—â–µ)
  def writeBatchInsert(points: Chunk[GpsEvent]): ConnectionIO[Int] = {
    val sql = """
      INSERT INTO gps_points (
        time, vehicle_id, latitude, longitude, altitude,
        speed, course, satellites, protocol, is_valid,
        validation_error, is_moving, sensors, received_at, instance_id
      ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?::jsonb, ?, ?)
    """
    
    Update[GpsEvent](sql).updateMany(points.toList)
  }
}
```

### –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å COPY vs INSERT

| –ú–µ—Ç–æ–¥ | 500 —Ç–æ—á–µ–∫ | 1000 —Ç–æ—á–µ–∫ | 5000 —Ç–æ—á–µ–∫ |
|-------|-----------|------------|------------|
| **COPY** | 5ms | 8ms | 25ms |
| **Batch INSERT** | 15ms | 28ms | 120ms |
| **Single INSERT** | 250ms | 500ms | 2500ms |

---

## –û—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å

### Exactly-Once Semantics

```mermaid
sequenceDiagram
    participant K as Kafka
    participant HW as History Writer
    participant DB as TimescaleDB

    Note over K,DB: –ì–∞—Ä–∞–Ω—Ç–∏—è exactly-once

    HW->>K: Poll messages (offsets 100-150)
    HW->>HW: Buffer points
    
    alt Flush —É—Å–ø–µ—à–µ–Ω
        HW->>DB: BEGIN
        HW->>DB: COPY 50 points
        HW->>DB: COMMIT
        HW->>K: Commit offsets (150)
        Note over K,DB: ‚úÖ –¢–æ—á–∫–∏ –∑–∞–ø–∏—Å–∞–Ω—ã, offset —Å–¥–≤–∏–Ω—É—Ç
    else Flush –Ω–µ—É—Å–ø–µ—à–µ–Ω (DB error)
        HW->>DB: BEGIN
        HW->>DB: COPY 50 points
        DB-->>HW: ERROR (connection lost)
        HW->>DB: ROLLBACK
        Note over HW: –ù–ï –∫–æ–º–º–∏—Ç–∏–º offset –≤ Kafka!
        HW->>HW: Retry –ø–æ—Å–ª–µ –ø–∞—É–∑—ã
        Note over K,DB: ‚ö†Ô∏è –ü—Ä–∏ —Ä–µ—Å—Ç–∞—Ä—Ç–µ –ø–µ—Ä–µ—á–∏—Ç–∞–µ–º —Ç–µ –∂–µ —Å–æ–æ–±—â–µ–Ω–∏—è
    end
```

### –ò–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å

```sql
-- –£–Ω–∏–∫–∞–ª—å–Ω—ã–π constraint –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
-- (time, vehicle_id) —É–∂–µ —è–≤–ª—è–µ—Ç—Å—è PRIMARY KEY

-- –ü—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –∑–∞–ø–∏—Å–∏ —Ç–µ—Ö –∂–µ –¥–∞–Ω–Ω—ã—Ö:
INSERT INTO gps_points (...) VALUES (...)
ON CONFLICT (time, vehicle_id) DO NOTHING;

-- –ò–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ):
INSERT INTO gps_points (...) VALUES (...)
ON CONFLICT (time, vehicle_id) DO UPDATE SET
  speed = EXCLUDED.speed,
  sensors = EXCLUDED.sensors;
```

### Graceful Shutdown

```scala
object GracefulShutdown {
  
  def run(
    buffer: BatchBuffer,
    writer: BatchWriter,
    consumer: Consumer
  ): ZIO[Any, Throwable, Unit] = {
    
    val shutdown = for {
      _ <- ZIO.logInfo("Shutdown signal received")
      
      // 1. –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —á—Ç–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
      _ <- consumer.stopConsumption
      
      // 2. –î–æ–∂–¥–∞—Ç—å—Å—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—É—â–µ–≥–æ batch
      _ <- ZIO.sleep(1.second)
      
      // 3. Flush –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –¥–∞–Ω–Ω—ã—Ö
      remaining <- buffer.takeBatch
      _ <- ZIO.when(remaining.nonEmpty) {
        for {
          _ <- ZIO.logInfo(s"Final flush: ${remaining.size} points")
          _ <- writer.writeBatch(remaining)
          offsets <- buffer.getOffsets
          _ <- consumer.commit(offsets)
        } yield ()
      }
      
      // 4. –ó–∞–∫—Ä—ã—Ç—å consumer
      _ <- consumer.close
      _ <- ZIO.logInfo("Shutdown complete")
    } yield ()
    
    shutdown
  }
}
```

---

## –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ

### –ü–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Kafka Topic: gps-events                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  P0  ‚îÇ  P1  ‚îÇ  P2  ‚îÇ  P3  ‚îÇ  P4  ‚îÇ  P5  ‚îÇ  P6  ‚îÇ  P7  ‚îÇ  P8  ‚îÇ P9-11‚îÇ
‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
   ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ
   ‚ñº      ‚ñº      ‚ñº      ‚ñº      ‚ñº      ‚ñº      ‚ñº      ‚ñº      ‚ñº      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  HW-1        ‚îÇ ‚îÇ  HW-2        ‚îÇ ‚îÇ  HW-3        ‚îÇ ‚îÇ  HW-4        ‚îÇ
‚îÇ  P0, P1, P2  ‚îÇ ‚îÇ  P3, P4, P5  ‚îÇ ‚îÇ  P6, P7, P8  ‚îÇ ‚îÇ  P9, P10, P11‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ              ‚îÇ              ‚îÇ              ‚îÇ
         ‚ñº              ‚ñº              ‚ñº              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         TimescaleDB                                 ‚îÇ
‚îÇ                    (–≤—Å–µ –ø–∏—à—É—Ç –≤ –æ–¥–Ω—É —Ç–∞–±–ª–∏—Ü—É)                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### –ü—Ä–∞–≤–∏–ª–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è

| –£—Å–ª–æ–≤–∏–µ | –î–µ–π—Å—Ç–≤–∏–µ |
|---------|----------|
| –ü–∞—Ä—Ç–∏—Ü–∏–π > Consumers | –î–æ–±–∞–≤–∏—Ç—å consumer |
| Consumer lag > 10000 | Scale up |
| CPU < 20% –Ω–∞ –≤—Å–µ—Ö | Scale down |
| DB latency > 100ms | –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ë–î |

### Connection Pool

```scala
// HikariCP –¥–ª—è connection pooling
val hikariConfig = new HikariConfig()
hikariConfig.setJdbcUrl("jdbc:postgresql://timescaledb:5432/tracker")
hikariConfig.setUsername("postgres")
hikariConfig.setPassword(sys.env("DB_PASSWORD"))
hikariConfig.setMaximumPoolSize(10)        // –ù–∞ –∏–Ω—Å—Ç–∞–Ω—Å
hikariConfig.setMinimumIdle(2)
hikariConfig.setConnectionTimeout(30000)
hikariConfig.setIdleTimeout(600000)
hikariConfig.setMaxLifetime(1800000)

// –û–±—â–∏–π –ø—É–ª –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä: 10 connections √ó 4 instances = 40 connections
// TimescaleDB default max_connections = 100
```

---

## –ú–µ—Ç—Ä–∏–∫–∏ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

### Prometheus –º–µ—Ç—Ä–∏–∫–∏

```
# Consumer lag
hw_consumer_lag{partition="0"} 150
hw_consumer_lag{partition="1"} 89
hw_consumer_lag_total 1234

# Buffer
hw_buffer_size 487
hw_buffer_capacity 10000
hw_buffer_utilization 0.0487

# Flush operations
hw_flush_total 5678
hw_flush_duration_seconds_bucket{le="0.01"} 5500
hw_flush_duration_seconds_bucket{le="0.1"} 5670
hw_flush_points_total 2839000

# Database
hw_db_insert_duration_seconds_bucket{le="0.01"} 5400
hw_db_insert_errors_total 12
hw_db_connection_pool_active 3
hw_db_connection_pool_idle 7

# Throughput
hw_points_per_second 8500
hw_bytes_per_second 425000
```

### Grafana Dashboard

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    History Writer Dashboard                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Points/sec           ‚îÇ   Consumer Lag        ‚îÇ   Buffer Size       ‚îÇ
‚îÇ       [8,456]         ‚îÇ      [1,234]          ‚îÇ      [487]          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  Throughput over time                                               ‚îÇ
‚îÇ   12K ‚î§                    ‚ï≠‚îÄ‚ïÆ                                      ‚îÇ
‚îÇ   10K ‚î§               ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ                                ‚îÇ
‚îÇ    8K ‚î§          ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ          ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ                            ‚îÇ
‚îÇ    6K ‚î§     ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ                    ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ                        ‚îÇ
‚îÇ    4K ‚î§‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ                                                      ‚îÇ
‚îÇ     0 ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                    ‚îÇ
‚îÇ       00:00  04:00  08:00  12:00  16:00  20:00                     ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Flush Duration p99   ‚îÇ   DB Latency p99      ‚îÇ   Instance Count    ‚îÇ
‚îÇ      [8.2 ms]         ‚îÇ      [5.1 ms]         ‚îÇ      [3]            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  Consumer Lag by Partition                                          ‚îÇ
‚îÇ  P0  ‚ñà‚ñà 150                                                        ‚îÇ
‚îÇ  P1  ‚ñà 89                                                          ‚îÇ
‚îÇ  P2  ‚ñà‚ñà‚ñà 245                                                       ‚îÇ
‚îÇ  P3  ‚ñà 67                                                          ‚îÇ
‚îÇ  ... (–æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–∞—Ä—Ç–∏—Ü–∏–∏)                                          ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### –ê–ª–µ—Ä—Ç—ã

```yaml
groups:
  - name: history-writer
    rules:
      - alert: HWConsumerLagHigh
        expr: sum(hw_consumer_lag) > 50000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Consumer lag is high ({{ $value }})"

      - alert: HWConsumerLagCritical
        expr: sum(hw_consumer_lag) > 200000
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Consumer lag is critical!"

      - alert: HWFlushErrors
        expr: rate(hw_flush_errors_total[5m]) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Flush errors detected"

      - alert: HWDBLatencyHigh
        expr: histogram_quantile(0.99, hw_db_insert_duration_seconds_bucket) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "DB insert latency > 100ms"
```

---

## –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

### application.conf

```hocon
history-writer {
  instance-id = ${?HOSTNAME}
  instance-id = ${?HW_INSTANCE_ID}
  
  kafka {
    bootstrap-servers = ${KAFKA_BROKERS}
    group-id = "history-writer-group"
    topic = "gps-events"
    
    consumer {
      max-poll-records = 1000
      fetch-min-bytes = 1024
      fetch-max-wait-ms = 500
      session-timeout-ms = 30000
      auto-offset-reset = "earliest"
    }
  }
  
  buffer {
    batch-size = 500
    flush-interval = 5s
    max-queue-size = 10000
  }
  
  database {
    url = ${DATABASE_URL}
    driver = "org.postgresql.Driver"
    
    pool {
      max-size = 10
      min-idle = 2
      connection-timeout = 30s
      idle-timeout = 10m
      max-lifetime = 30m
    }
    
    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å COPY –≤–º–µ—Å—Ç–æ INSERT
    use-copy = true
  }
  
  admin {
    port = 8091
    enabled = true
  }
  
  retry {
    max-attempts = 3
    initial-delay = 100ms
    max-delay = 5s
    multiplier = 2.0
  }
}
```

### Environment variables

```bash
# –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ
KAFKA_BROKERS=kafka:9092
DATABASE_URL=jdbc:postgresql://timescaledb:5432/tracker
DB_PASSWORD=secret

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ
HW_INSTANCE_ID=hw-instance-1
HW_BATCH_SIZE=500
HW_FLUSH_INTERVAL=5s
HW_ADMIN_PORT=8091

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json
```

### Docker Compose

```yaml
services:
  history-writer:
    build: ./services/history-writer
    ports:
      - "8091:8091"  # Admin API
    environment:
      - KAFKA_BROKERS=kafka:9092
      - DATABASE_URL=jdbc:postgresql://timescaledb:5432/tracker
      - DB_PASSWORD=${DB_PASSWORD}
      - HW_INSTANCE_ID=hw-1
    depends_on:
      - kafka
      - timescaledb
    deploy:
      replicas: 3  # 3 –∏–Ω—Å—Ç–∞–Ω—Å–∞ –¥–ª—è 12 –ø–∞—Ä—Ç–∏—Ü–∏–π
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8091/health"]
      interval: 10s
      timeout: 5s
      retries: 3
```

---

## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å Connection Manager

| –ü–∞—Ä–∞–º–µ—Ç—Ä | Connection Manager | History Writer |
|----------|-------------------|----------------|
| **–í—Ö–æ–¥** | TCP (—Ç—Ä–µ–∫–µ—Ä—ã) | Kafka |
| **–í—ã—Ö–æ–¥** | Kafka + Redis | TimescaleDB |
| **State** | Stateless | Stateless |
| **Scaling** | –ü–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è–º | –ü–æ –ø–∞—Ä—Ç–∏—Ü–∏—è–º |
| **–ö—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å** | –í—ã—Å–æ–∫–∞—è (realtime) | –°—Ä–µ–¥–Ω—è—è (async) |
| **Latency** | < 50ms | < 10s |

---

## üìö –°–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã

- [ARCHITECTURE_BLOCK1.md](../ARCHITECTURE_BLOCK1.md) ‚Äî –û–±–∑–æ—Ä Block 1
- [CONNECTION_MANAGER.md](./CONNECTION_MANAGER.md) ‚Äî –ü—Ä–µ–¥—ã–¥—É—â–∏–π —Å–µ—Ä–≤–∏—Å
- [DEVICE_MANAGER.md](./DEVICE_MANAGER.md) ‚Äî –°–ª–µ–¥—É—é—â–∏–π —Å–µ—Ä–≤–∏—Å
- [DATA_STORES.md](../DATA_STORES.md) ‚Äî –°—Ö–µ–º—ã —Ö—Ä–∞–Ω–∏–ª–∏—â

---

## ü§ñ –ü—Ä–æ–º–ø—Ç –¥–ª—è AI-–∞–≥–µ–Ω—Ç–∞

<details>
<summary><b>–†–∞–∑–≤–µ—Ä–Ω—É—Ç—å –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ History Writer</b></summary>

```markdown
# –ó–ê–î–ê–ß–ê: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å History Writer –¥–ª—è TrackerGPS

## –ö–û–ù–¢–ï–ö–°–¢
–¢—ã ‚Äî senior Scala —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫. –°–æ–∑–¥–∞–π History Writer ‚Äî —Å–µ—Ä–≤–∏—Å –∑–∞–ø–∏—Å–∏ GPS –∏—Å—Ç–æ—Ä–∏–∏ –≤ TimescaleDB –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞ TrackerGPS.

## –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ô –°–¢–ï–ö (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û)
- **–Ø–∑—ã–∫:** Scala 3.4.0
- **–≠—Ñ—Ñ–µ–∫—Ç—ã:** ZIO 2.0.20
- **Kafka:** zio-kafka (consumer)
- **PostgreSQL:** Quill + JDBC (–¥–ª—è COPY –∫–æ–º–∞–Ω–¥—ã)
- **–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:** zio-config + HOCON
- **–ú–µ—Ç—Ä–∏–∫–∏:** zio-metrics + Prometheus
- **–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ:** zio-logging + SLF4J
- **–°–±–æ—Ä–∫–∞:** SBT

## –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –°–ï–†–í–ò–°–ê

### –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:
1. **Kafka Consumer** ‚Äî —á–∏—Ç–∞–µ—Ç –∏–∑ —Ç–æ–ø–∏–∫–∞ `gps-events`
2. **Buffer** ‚Äî in-memory –±—É—Ñ–µ—Ä –¥–ª—è batch accumulation
3. **Batch Writer** ‚Äî COPY INSERT –≤ TimescaleDB
4. **Deduplicator** ‚Äî —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (device_id + timestamp)
5. **HTTP API** ‚Äî health, metrics, lag info

### Flow –æ–±—Ä–∞–±–æ—Ç–∫–∏:
```
Kafka (gps-events) ‚Üí Deserialize ‚Üí Deduplicate ‚Üí Buffer ‚Üí Batch COPY ‚Üí TimescaleDB
                                                    ‚Üì
                                              Flush triggers:
                                              - size >= 500
                                              - time >= 5s
                                              - shutdown
```

## –¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –†–ï–ê–õ–ò–ó–ê–¶–ò–ò

### 1. Kafka Consumer
```scala
trait GpsEventConsumer:
  // –ü–æ—Ç–æ–∫ —Å–æ–±—ã—Ç–∏–π –∏–∑ Kafka
  def stream: ZStream[Any, Throwable, CommittableRecord[String, GpsEvent]]

// –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:
// - Topic: gps-events
// - Consumer group: history-writer-group
// - Auto-offset-reset: earliest
// - Enable.auto.commit: false (manual commit –ø–æ—Å–ª–µ –∑–∞–ø–∏—Å–∏ –≤ DB)
// - Max.poll.records: 1000
```

### 2. Buffer –∏ Batch Processing
```scala
trait BatchBuffer:
  // –î–æ–±–∞–≤–∏—Ç—å —Ç–æ—á–∫—É –≤ –±—É—Ñ–µ—Ä
  def add(point: GpsPoint): UIO[Unit]
  
  // –ü–æ–ª—É—á–∏—Ç—å –∏ –æ—á–∏—Å—Ç–∏—Ç—å –±—É—Ñ–µ—Ä (–∞—Ç–æ–º–∞—Ä–Ω–æ)
  def flush: UIO[Chunk[GpsPoint]]
  
  // –¢–µ–∫—É—â–∏–π —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞
  def size: UIO[Int]

// –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ Ref[Chunk[GpsPoint]]
// Flush triggers:
// 1. Buffer size >= batchSize (default 500)
// 2. Time since last flush >= flushInterval (default 5s)
// 3. Shutdown signal
```

### 3. TimescaleDB Writer
```scala
trait HistoryWriter:
  // Batch insert —á–µ—Ä–µ–∑ COPY
  def writeBatch(points: Chunk[GpsPoint]): Task[Int]

// –í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PostgreSQL COPY –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
// –ü—Ä–∏–º–µ—Ä:
// COPY gps_positions (device_id, timestamp, latitude, longitude, ...)
// FROM STDIN WITH (FORMAT binary)
```

### 4. –ú–æ–¥–µ–ª—å –¥–∞–Ω–Ω—ã—Ö
```scala
case class GpsPoint(
  deviceId: String,
  timestamp: Instant,
  latitude: Double,
  longitude: Double,
  altitude: Option[Int],
  speed: Option[Int],
  course: Option[Int],
  satellites: Option[Int],
  hdop: Option[Double],
  ignition: Option[Boolean],
  fuelLevel: Option[Double],
  odometer: Option[Long],
  ioData: JsonObject,        // JSONB –≤ PostgreSQL
  receivedAt: Instant        // –í—Ä–µ–º—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–µ—Ä–≤–µ—Ä–æ–º
)
```

### 5. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
```scala
trait Deduplicator:
  // –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–æ—á–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–º
  def isDuplicate(deviceId: String, timestamp: Instant): UIO[Boolean]
  
  // –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–æ—á–∫—É
  def register(deviceId: String, timestamp: Instant): UIO[Unit]

// –†–µ–∞–ª–∏–∑–∞—Ü–∏—è: Bloom filter –∏–ª–∏ LRU cache
// –ö–ª—é—á: (device_id, timestamp)
// TTL: 1 —á–∞—Å (—Ç–æ—á–∫–∏ —Å—Ç–∞—Ä—à–µ –Ω–µ –ø—Ä–∏—Ö–æ–¥—è—Ç)
```

### 6. Graceful Shutdown
```scala
// –ü—Ä–∏ shutdown:
// 1. –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å consumer (–ø–µ—Ä–µ—Å—Ç–∞—Ç—å —á–∏—Ç–∞—Ç—å)
// 2. –î–æ–∂–¥–∞—Ç—å—Å—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—É—â–µ–≥–æ batch
// 3. Flush –æ—Å—Ç–∞–≤—à–∏–π—Å—è –±—É—Ñ–µ—Ä
// 4. Commit offsets
// 5. –ó–∞–∫—Ä—ã—Ç—å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
```

### 7. HTTP API
```scala
// GET /health ‚Äî liveness
// GET /ready ‚Äî readiness (Kafka + DB connected)
// GET /metrics ‚Äî Prometheus
// GET /lag ‚Äî —Ç–µ–∫—É—â–∏–π consumer lag
// GET /stats ‚Äî —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (points/sec, batch sizes, etc.)
```

### 8. –ú–µ—Ç—Ä–∏–∫–∏
```scala
// Counters
hw_events_received_total
hw_events_written_total
hw_events_deduplicated_total
hw_batches_written_total

// Gauges
hw_buffer_size
hw_consumer_lag

// Histograms
hw_batch_size_bucket{le="100"}
hw_batch_size_bucket{le="500"}
hw_write_duration_seconds_bucket{le="0.1"}
hw_write_duration_seconds_bucket{le="1.0"}
```

## –°–¢–†–£–ö–¢–£–†–ê –ü–†–û–ï–ö–¢–ê
```
history-writer/
‚îú‚îÄ‚îÄ src/main/scala/
‚îÇ   ‚îî‚îÄ‚îÄ trackergps/historywriter/
‚îÇ       ‚îú‚îÄ‚îÄ Main.scala
‚îÇ       ‚îú‚îÄ‚îÄ config/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ AppConfig.scala
‚îÇ       ‚îú‚îÄ‚îÄ kafka/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ GpsEventConsumer.scala
‚îÇ       ‚îú‚îÄ‚îÄ buffer/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ BatchBuffer.scala
‚îÇ       ‚îú‚îÄ‚îÄ db/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ HistoryWriter.scala
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ CopyWriter.scala     # PostgreSQL COPY
‚îÇ       ‚îú‚îÄ‚îÄ dedup/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ Deduplicator.scala
‚îÇ       ‚îú‚îÄ‚îÄ http/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ HealthApi.scala
‚îÇ       ‚îî‚îÄ‚îÄ metrics/
‚îÇ           ‚îî‚îÄ‚îÄ Metrics.scala
‚îú‚îÄ‚îÄ src/main/resources/
‚îÇ   ‚îî‚îÄ‚îÄ application.conf
‚îî‚îÄ‚îÄ build.sbt
```

## –ü–†–ò–ú–ï–† –ö–û–î–ê

```scala
// Main processing loop
def processStream: ZIO[GpsEventConsumer & BatchBuffer & HistoryWriter, Throwable, Unit] =
  for
    consumer <- ZIO.service[GpsEventConsumer]
    buffer   <- ZIO.service[BatchBuffer]
    writer   <- ZIO.service[HistoryWriter]
    
    // –§–æ–Ω–æ–≤—ã–π flush –ø–æ —Ç–∞–π–º–µ—Ä—É
    _ <- (ZIO.sleep(5.seconds) *> flushBuffer).forever.fork
    
    // –û—Å–Ω–æ–≤–Ω–æ–π –ø–æ—Ç–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    _ <- consumer.stream
          .mapZIO { record =>
            for
              point <- ZIO.fromEither(record.value.toGpsPoint)
              _     <- buffer.add(point)
              size  <- buffer.size
              _     <- ZIO.when(size >= 500)(flushBuffer)
              _     <- record.offset.commit
            yield ()
          }
          .runDrain
  yield ()

def flushBuffer: ZIO[BatchBuffer & HistoryWriter, Throwable, Unit] =
  for
    buffer <- ZIO.service[BatchBuffer]
    writer <- ZIO.service[HistoryWriter]
    points <- buffer.flush
    _      <- ZIO.when(points.nonEmpty) {
                writer.writeBatch(points)
                  .tap(n => ZIO.logInfo(s"Written $n points"))
              }
  yield ()
```

## PostgreSQL COPY Implementation

```scala
// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CopyManager –∏–∑ PostgreSQL JDBC
def writeBatchCopy(points: Chunk[GpsPoint]): Task[Int] = ZIO.attempt {
  val conn = dataSource.getConnection()
  try {
    val copyManager = new CopyManager(conn.unwrap(classOf[BaseConnection]))
    
    val sql = """
      COPY gps_positions (device_id, timestamp, latitude, longitude, altitude, 
                          speed, course, satellites, ignition, fuel_level, io_data)
      FROM STDIN WITH (FORMAT csv, NULL 'NULL')
    """
    
    val reader = new StringReader(points.map(toCsvLine).mkString("\n"))
    copyManager.copyIn(sql, reader).toInt
  } finally {
    conn.close()
  }
}
```

## –ö–†–ò–¢–ï–†–ò–ò –ü–†–ò–Å–ú–ö–ò

1. ‚úÖ Consumer —á–∏—Ç–∞–µ—Ç –∏–∑ Kafka —Å manual commit
2. ‚úÖ Batch accumulation —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ size –∏ time
3. ‚úÖ COPY INSERT –≤ TimescaleDB (–Ω–µ –æ–±—ã—á–Ω—ã–π INSERT)
4. ‚úÖ –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ (device_id, timestamp)
5. ‚úÖ Graceful shutdown –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –¥–∞–Ω–Ω—ã—Ö
6. ‚úÖ Consumer lag –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö
7. ‚úÖ Unit —Ç–µ—Å—Ç—ã –¥–ª—è buffer –∏ dedup
8. ‚úÖ Integration test —Å embedded Kafka + PostgreSQL
```

</details>

---

**–î–∞—Ç–∞:** 26 —è–Ω–≤–∞—Ä—è 2026  
**–°—Ç–∞—Ç—É—Å:** –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –≥–æ—Ç–æ–≤–∞ ‚úÖ
